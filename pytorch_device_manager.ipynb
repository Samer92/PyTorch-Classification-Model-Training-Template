{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_device_manager.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Rflk82pxfavy","colab_type":"code","colab":{}},"source":["import torch \n","from contextlib import contextmanager\n","import gc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyut1KDIfchO","colab_type":"code","colab":{}},"source":["class DeviceManager:\n","    \"\"\" # To get info about the available device to train on \"\"\"\n","    def __init__(self):\n","        self.rounding_digits = 3\n","        \n","    def available_gpus_info(self):\n","        gpus_count = torch.cuda.device_count()\n","        print('Number of GPUs : ', gpus_count, '\\n')\n","        \n","        for i in range(gpus_count):\n","            gpu_props = torch.cuda.get_device_properties(i)\n","            gpu_name = gpu_props.name\n","            gpu_memory = round(gpu_props.total_memory * 1e-9, self.rounding_digits)\n","            \n","            print('* GPU index : {} \\t GPU name : {} \\t RAM : {} [GB] \\n'.format(i, gpu_name, gpu_memory))\n","    \n","    def get_gpu_device(self, gpu_id):\n","        device =  torch.device(\"cuda:{}\".format(gpu_id))\n","        return device\n","    \n","    def get_gpu_memory_allocated(self):\n","        gpu_memory_allocated = round(torch.cuda.memory_allocated() * 1e-6, self.rounding_digits)\n","        print('Current GPU memory allocated {} [MB] GPU RAM'.format(gpu_memory_allocated))\n","        \n","    @contextmanager\n","    def get_last_gpu_usage(self, desc_str=' '):\n","        \"\"\" To test an operation how much it could consume GPU memory \"\"\"\n","        pre_gpu_memory_allocated = torch.cuda.memory_allocated()\n","        yield None\n","        post_gpu_memory_allocated = torch.cuda.memory_allocated()\n","        last_gpu_memory_allocated = (post_gpu_memory_allocated - pre_gpu_memory_allocated) * 1e-6\n","        last_gpu_memory_allocated = round(last_gpu_memory_allocated, self.rounding_digits)\n","        print(desc_str + ' reserved {} [MB] GPU RAM'.format(last_gpu_memory_allocated))\n","        \n","    def tensors_tracking(self):\n","        \"\"\" To track the current defined tensors in the memory \"\"\"\n","        for obj in gc.get_objects():\n","            try:\n","                if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n","                    print(type(obj), obj.size())\n","            except: \n","                pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXQRY5uifdim","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
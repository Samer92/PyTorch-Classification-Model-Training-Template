{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"evaluation_metrics.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"1xh-Yof0vsbQ","colab":{}},"source":["from sklearn import datasets, svm\n","import numpy as np\n","import pandas as pd\n","import copy\n","import pickle as pk\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","from imblearn.metrics import specificity_score\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.model_selection import train_test_split\n","from scipy import interp\n","from matplotlib import pyplot as plt\n","import seaborn as sns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1nPwdWiDvsby","colab":{}},"source":["# 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n","# 'macro': Calculate metrics for each label, and find their unweighted mean\n","# 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"id63xGaQvscF","colab":{}},"source":["# # Import some data to play with\n","# dataset = datasets.load_iris()\n","# #dataset = datasets.load_breast_cancer()\n","# X = dataset.data\n","# y = dataset.target\n","# classes_label = datasets\n","\n","# # Binarize the output\n","# y = label_binarize(y, classes=list(range(dataset.target_names.__len__())))\n","# n_classes = y.shape[1]\n","\n","# if n_classes + 1 == 2:\n","#     y = np.append(y, 1-y,axis=1)\n","\n","# # Add noisy features to make the problem harder\n","# random_state = np.random.RandomState(0)\n","# # n_samples, n_features = X.shape\n","# # X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n","\n","# # shuffle and split training and test sets\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.8,\n","#                                                     random_state=0)\n","\n","# # Learn to predict each class against the other\n","# classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n","#                                  random_state=random_state))\n","# y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n","\n","# y_pred = classifier.predict(X_test).argmax(axis=1)\n","# y_true = y_test.argmax(axis=1)\n","\n","# with open('test.pkl', 'rb') as f:\n","#     data = pk.load(f)\n","\n","# y_score = data['y_score']\n","# y_pred = data['y_pred']\n","# y_true = data['y_true']\n","# classes_labels = data['classes_labels']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U6orn43bvscb","colab":{}},"source":["def plot_confusion_matrix(confusion_mat, \n","                          classes, \n","                          figure_axis,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    \"\"\"\n","    \n","    # Compute confusion matrix\n","    cm = confusion_mat\n","\n","    ax = figure_axis\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.grid(False)\n","    ax.figure.colorbar(im, ax=ax)\n","    \n","    if classes is not None:\n","        # We want to show all ticks...\n","        ax.set(xticks=np.arange(cm.shape[1]),\n","               yticks=np.arange(cm.shape[0]),\n","               # ... and label them with the respective list entries\n","               xticklabels=classes, yticklabels=classes,\n","               ylabel='True Label',\n","               xlabel='Predicted Label')\n","\n","        # Rotate the tick labels and set their alignment.\n","        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","    else:\n","        plt.setp( ax.get_xticklabels(), visible=False)\n","        plt.setp( ax.get_yticklabels(), visible=False)\n","        plt.setp( ax.get_xticklines(), visible=False)\n","        plt.setp( ax.get_yticklines(), visible=False)\n","        \n","    ax.set(title=title)\n","\n","    # Loop over data dimensions and create text annotations.\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, cm[i, j],\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","\n","def plot_confusion_table_sample(figure_axis, cmap=plt.cm.Blues):\n","    cm = np.array([[1, 0.5],\n","                   [0.5, 1]])\n","    \n","    labels = [['TP', 'FP'],\n","              ['FN', 'TN']]\n","    \n","    ax=figure_axis\n","    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.grid(False)\n","    plt.setp( ax.get_xticklabels(), visible=False)\n","    plt.setp( ax.get_yticklabels(), visible=False)\n","    plt.setp( ax.get_xticklines(), visible=False)\n","    plt.setp( ax.get_yticklines(), visible=False)\n","    \n","    # Loop over data dimensions and create text annotations.\n","    thresh = cm.max() / 2.\n","    \n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, labels[i][j],\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WTIG67EZvsco","colab":{}},"source":["class ClassifierReport:\n","\n","    def __init__(self, y_true, y_pred, y_score , number_of_classes, average_type = 'macro', \n","                 digits_count_fp = 3,classes_labels = None):\n","        \"\"\" Initialization function\n","            y_true: list or numpy array (number of samples)\n","            y_pred: list or numpy array (number of samples)\n","            y_score: numpy array contains the actual outputs before decision (number of samples, number of classes)\n","            average_type: determine how to calculate the overall metrics\n","            digits_count_fp: number of digits after the floating point\n","            classes_labels: list of classes labels\n","        \"\"\"\n","        \n","        self.y_true = np.array(y_true)\n","        self.y_pred = np.array(y_pred)\n","        self.y_score = np.array(y_score)\n","        self.number_of_classes = number_of_classes\n","        self.y_true_one_hot = label_binarize(y_true, classes=list(range(self.number_of_classes)))\n","        \n","        self.number_of_samples = len(self.y_true)\n","        self.number_of_samples_per_class = [(self.y_true==c).sum() \n","                                for c in range(self.number_of_classes)]\n","        \n","        self.classes_labels = ['Class ' + str(c) for c in range(self.number_of_classes)] \\\n","                                if classes_labels is None \\\n","                                else classes_labels\n","        \n","        self.digits_count_fp = digits_count_fp\n","        self.average_type = average_type\n","        \n","        self.TP_TN_FP_FN()\n","        self.calculate_confusion_matrix()\n","        self.calculate_confusion_tables()\n","        self.accuracy()\n","        self.recall()\n","        self.precision()\n","        self.f1_score()\n","        self.specificity()\n","        self.cohen_kappa()\n","        self.calculate_roc_auc()\n","        \n","        \n","    def TP_TN_FP_FN(self):\n","        self.TP = np.zeros(self.number_of_classes)\n","        self.FP = np.zeros(self.number_of_classes)\n","        self.TN = np.zeros(self.number_of_classes)\n","        self.FN = np.zeros(self.number_of_classes)\n","        \n","        for cls in range(self.number_of_classes):\n","            # Calculate\n","            self.TP[cls] = (self.y_pred[self.y_true == cls] == cls).sum()\n","            self.FN[cls] = (self.y_pred[self.y_true == cls] != cls).sum()\n","            \n","            self.TN[cls] = (self.y_pred[self.y_true != cls] != cls).sum()\n","            self.FP[cls] = (self.y_pred[self.y_true != cls] == cls).sum()            \n","    \n","    def calculate_confusion_matrix(self):\n","        \"\"\" Function to calculate confusion matrix and weighted confusion matrix \"\"\"\n","        self.confusion_matrix = confusion_matrix(self.y_true, self.y_pred)\n","        \n","        classes_weights = np.array(self.number_of_samples_per_class).reshape(\n","            self.number_of_classes, 1)\n","        \n","        self.normalized_confusion_matrix = (self.confusion_matrix/classes_weights).round(self.digits_count_fp)\n","    \n","    def calculate_confusion_tables(self):\n","        \"\"\" Function to calculate confusion table and weighted confusion table \n","            for each class\n","        \"\"\"\n","        self.confusion_tables = np.zeros((self.number_of_classes, 2, 2))\n","        self.normalized_confusion_tables = np.zeros((self.number_of_classes, 2, 2))\n","        \n","        for cls in range(self.number_of_classes):\n","            # Normal confusion table\n","            self.confusion_tables[cls, 0, 0] = self.TP[cls] # TP\n","            self.confusion_tables[cls, 0, 1] = self.FP[cls] # FP\n","            self.confusion_tables[cls, 1, 0] = self.FN[cls] # FN\n","            self.confusion_tables[cls, 1, 1] = self.TN[cls] # TN\n","            \n","            # Weighted confusion table\n","            table_weights = self.confusion_tables[cls].sum(axis=0).reshape(1, 2)\n","            self.normalized_confusion_tables[cls] = (self.confusion_tables[cls]/table_weights).round(self.digits_count_fp)\n","        \n","        # Convert the data type into int\n","        self.confusion_tables = self.confusion_tables.astype(int)\n","    \n","    def accuracy(self, sample_weight = None):\n","        \"\"\" Refer to sklearn for full doc\"\"\"\n","        if sample_weight is None:\n","            sample_weight  = np.ones(self.number_of_samples)\n","        self.overall_accuracy = accuracy_score(\n","            self.y_true, self.y_pred, sample_weight=sample_weight).round(self.digits_count_fp)\n","        \n","    def recall(self):\n","        \"\"\"Recall is also known as Sensitivity and True Positive Rate\"\"\"\n","        self.overall_recall = recall_score(\n","            self.y_true, self.y_pred, average = self.average_type).round(self.digits_count_fp)\n","        self.classes_recall = recall_score(\n","            self.y_true, self.y_pred, average = None).round(self.digits_count_fp)\n","        \n","    def precision(self):\n","        \"\"\" Precision or Positive Predictive Value \"\"\"\n","        self.overall_precision = precision_score(\n","            self.y_true, self.y_pred, average = self.average_type).round(self.digits_count_fp)\n","        self.classes_precision = precision_score(\n","            self.y_true, self.y_pred, average = None).round(self.digits_count_fp)\n","    \n","    def f1_score(self):\n","        \"\"\" f1_score is harmonic mean of recall and precision\"\"\"\n","        self.overall_f1_score = f1_score(\n","            self.y_true, self.y_pred, average = self.average_type).round(self.digits_count_fp)\n","        self.classes_f1_score = f1_score(\n","            self.y_true, self.y_pred, average = None).round(self.digits_count_fp)\n","    \n","    \n","    def specificity(self):\n","        \"\"\" Specificity is also known as True Negative Rate \"\"\"\n","        self.overall_specificity = specificity_score(\n","            self.y_true, self.y_pred, average = self.average_type).round(self.digits_count_fp)\n","        self.classes_specificity = specificity_score(\n","            self.y_true, self.y_pred, average = None).round(self.digits_count_fp)\n","    \n","    def cohen_kappa(self):\n","        self.overall_cohen_kappa = cohen_kappa_score(self.y_true, self.y_pred).round(self.digits_count_fp)    \n","        \n","    def calculate_roc_auc(self):\n","        \"\"\" Refer to : https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n","        \"\"\"\n","        if self.number_of_classes == 2:\n","            self.fpr, self.tpr, self.thresholds = roc_curve(self.y_true, self.y_score[:,1])\n","            self.roc_auc = auc(self.fpr, self.tpr).round(self.digits_count_fp)\n","            \n","            # Rounding\n","            self.fpr = self.fpr.round(self.digits_count_fp)\n","            self.tpr = self.tpr.round(self.digits_count_fp)\n","            self.thresholds = self.thresholds.round(self.digits_count_fp)\n","            return \n","        \n","        # Compute ROC curve and ROC area for each class\n","        self.fpr = dict()    # fpr: False positive rate\n","        self.tpr = dict()    # tpr: True positive rate\n","        self.roc_auc = dict()\n","        for i in range(self.number_of_classes):\n","            self.fpr[i], self.tpr[i], _ = roc_curve(\n","                self.y_true_one_hot[:, i], self.y_score[:, i])\n","            self.roc_auc[i] = auc(self.fpr[i] , self.tpr[i]).round(\n","                self.digits_count_fp).round(self.digits_count_fp)\n","            \n","            # Rounding\n","            self.fpr[i] = self.fpr[i].round(self.digits_count_fp)\n","            self.tpr[i] = self.tpr[i].round(self.digits_count_fp)\n","\n","\n","        # Compute micro-average ROC curve and ROC area\n","        self.fpr[\"micro\"], self.tpr[\"micro\"], _ = roc_curve(\n","            self.y_true_one_hot.ravel(), self.y_score.ravel())\n","        self.roc_auc[\"micro\"] = auc(self.fpr[\"micro\"], self.tpr[\"micro\"]).round(self.digits_count_fp)\n","        \n","        # Rounding\n","        self.fpr[\"micro\"] = self.fpr[\"micro\"].round(self.digits_count_fp)\n","        self.tpr[\"micro\"] = self.tpr[\"micro\"].round(self.digits_count_fp)\n","        \n","        # Compute macro-average ROC curve and ROC area\n","        \n","        # First aggregate all false positive rates\n","        all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(self.number_of_classes)]))\n","\n","        # Then interpolate all ROC curves at this points\n","        mean_tpr = np.zeros_like(all_fpr)\n","        for i in range(self.number_of_classes):\n","            mean_tpr += interp(all_fpr, self.fpr[i], self.tpr[i])\n","\n","        # Finally average it and compute AUC\n","        mean_tpr /= self.number_of_classes\n","        \n","        # Rounding\n","        self.fpr[\"macro\"] = all_fpr.round(self.digits_count_fp)\n","        self.tpr[\"macro\"] = mean_tpr.round(self.digits_count_fp)\n","        self.roc_auc[\"macro\"] = auc(self.fpr[\"macro\"], self.tpr[\"macro\"]).round(self.digits_count_fp)\n","        \n","    def show_confusion_matrix(self):\n","        \n","        self.calculate_confusion_matrix()\n","        # fig = plt.figure(figsize=(8,3))\n","        fig = plt.figure(figsize=(10,4))\n","        ax = plt.subplot(1,2,1)\n","        plot_confusion_matrix(self.confusion_matrix, \n","                              self.classes_labels,\n","                              title = 'Confusion Matrix',\n","                              cmap = plt.cm.Blues,\n","                              figure_axis = ax)\n","        \n","        ax = plt.subplot(1,2,2)\n","        plot_confusion_matrix(self.normalized_confusion_matrix, \n","                              self.classes_labels,\n","                              title = 'Normalized Confusion Matrix',\n","                              cmap = plt.cm.Blues,\n","                              figure_axis = ax)\n","        fig.tight_layout()\n","        plt.show()\n","    \n","    def show_confusion_tables(self):\n","        self.calculate_confusion_tables()\n","        fig = plt.figure(figsize=(8,self.number_of_classes*2))\n","        table_counter = 0\n","        for cls in range(self.number_of_classes):\n","            table_counter += 1\n","            ax = plt.subplot(self.number_of_classes, 3, table_counter)\n","            plt.grid(False)\n","            plot_confusion_table_sample(ax)\n","            \n","            table_counter += 1\n","            ax = plt.subplot(self.number_of_classes, 3, table_counter)\n","            plot_confusion_matrix(self.confusion_tables[cls], \n","                              None,\n","                              title = self.classes_labels[cls],\n","                              cmap = plt.cm.Blues,\n","                              figure_axis = ax)\n","            \n","            table_counter += 1\n","            ax = plt.subplot(self.number_of_classes, 3, table_counter)\n","            plot_confusion_matrix(self.normalized_confusion_tables[cls], \n","                              None,\n","                              title = self.classes_labels[cls],\n","                              cmap = plt.cm.Blues,\n","                              figure_axis = ax)\n","        fig.tight_layout()\n","        plt.show()\n","        \n","    def show_overall_metrics(self, required_metrics = ['Accuracy', 'Recall', 'Precision', 'F1_score', 'Specificity', 'Cohen_Kappa']):\n","        # Overall\n","        overall_metrics = {\n","            'Accuracy': self.overall_accuracy,\n","            'Recall': self.overall_recall,\n","            'Precision': self.overall_precision,\n","            'F1_score': self.overall_f1_score,\n","            'Specificity': self.overall_specificity,\n","            'Cohen_Kappa': self.overall_cohen_kappa\n","            }\n","        \n","        overall_metrics = pd.Series(\n","                {m: overall_metrics[m] for m in required_metrics\n","                                       if m in overall_metrics.keys()})\n","        \n","        # Show overall metrics\n","        fig = plt.figure(figsize=(10, 5))\n","        ax = plt.subplot(1,2,1)\n","        overall_metrics.plot.bar()\n","        plt.title('Overall Metrics')\n","        plt.grid(True)\n","        plt.ylim((0,1.5))\n","        \n","        ax = plt.subplot(1,2,2)\n","        ax.axis('tight')\n","        ax.axis('off')\n","        table = ax.table(cellText = overall_metrics.values.reshape((len(overall_metrics),1)), \n","                     colLabels = ['Values'], \n","                     rowLabels = overall_metrics.index,\n","                     loc = 'center',\n","                     colWidths = [0.2]\n","                    )\n","        table.scale(2, 2)\n","        table.set_fontsize(12)\n","        plt.show()\n","        \n","    def show_classes_metrics(self, required_metrics = [ 'Recall', 'Precision', 'F1_score', 'Specificity', 'Cohen_Kappa']):\n","        # Per class\n","        classes_metrics = {\n","            'Recall': self.classes_recall,\n","            'Precision': self.classes_precision,\n","            'F1_score': self.classes_f1_score,\n","            'Specificity': self.classes_specificity\n","            }\n","        \n","        classes_metrics = pd.DataFrame(\n","            {m: classes_metrics[m] for m in required_metrics\n","                                   if m in classes_metrics.keys()})\n","        classes_metrics.index = self.classes_labels\n","        \n","        #fig = plt.figure(figsize=(10,5))\n","        # Data table\n","        ax = plt.subplot(1, 1, 1)\n","        ax.axis('tight')\n","        ax.axis('off')\n","        table = ax.table(cellText = classes_metrics.values, \n","                     colLabels = classes_metrics.columns, \n","                     rowLabels = classes_metrics.index,\n","                     loc = 'center',\n","                     colWidths = [0.2]*len(classes_metrics.columns))\n","        \n","        table.scale(2, 2)\n","        table.set_fontsize(15)\n","        plt.grid(True)\n","        plt.show()\n","        \n","        # Compare between metrics\n","        classes_metrics.plot.bar()\n","        plt.title('Metrics Comparison')\n","        plt.grid(True)\n","        plt.legend(loc=0)\n","        plt.ylim((0,1.5))\n","        plt.show()\n","        \n","        # Compare between classes\n","        #plt.subplot(3, 1, 3)\n","        classes_metrics.T.plot.bar()\n","        plt.title('Classes Comparison')\n","        plt.grid(True)\n","        plt.ylim((0,1.5))\n","        plt.show()\n","    \n","    def show_roc_curve(self):\n","        # Plot all ROC curves\n","        plt.figure(figsize=(7, 5))\n","        \n","        lw = 2\n","        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n","        if self.number_of_classes == 2:\n","            plt.plot(self.fpr, self.tpr,\n","                 label = 'ROC curve (area = {0:0.2f})'\n","                 ''.format(self.roc_auc),\n","                 color = 'crimson', linestyle = ':', linewidth = 4)\n","        else:\n","            \n","            # Micro average ROC\n","            plt.plot(self.fpr[\"micro\"], self.tpr[\"micro\"],\n","                 label='micro-average ROC curve (area = {0:0.2f})'\n","                 ''.format(self.roc_auc[\"micro\"]),\n","                 color='deeppink', linestyle=':', linewidth=4)\n","        \n","            # Macro average ROC\n","            plt.plot(self.fpr[\"macro\"], self.tpr[\"macro\"],\n","                 label='macro-average ROC curve (area = {0:0.2f})'\n","                 ''.format(self.roc_auc[\"macro\"]),\n","                 color = 'navy', linestyle = ':', linewidth = 4)\n","        \n","            # Classes ROC\n","        \n","            for i in range(self.number_of_classes):\n","                plt.plot(self.fpr[i], self.tpr[i],  lw=lw,\n","                 label='ROC curve of ({0}) (area = {1:0.2f})'\n","                 ''.format(self.classes_labels[i], self.roc_auc[i]))\n","\n","            \n","        plt.xlim([0.0, 1.0])\n","        plt.ylim([0.0, 1.05])\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('ROC Curves')\n","        plt.legend(loc=\"lower right\")\n","        plt.show()\n","    \n","    def show_all(self):\n","        new_lines = '\\n'\n","        \n","        print('Confusion Matrix' + new_lines)\n","        self.show_confusion_matrix()\n","        \n","        print(new_lines + 'Confusion Tables' + new_lines)\n","        self.show_confusion_tables()\n","        \n","        print(new_lines + 'Overall Metrics' + new_lines)\n","        self.show_overall_metrics()\n","        \n","        print(new_lines + 'Classes Metrics' + new_lines)\n","        self.show_classes_metrics()\n","        \n","        print(new_lines + 'ROC Curve' + new_lines)\n","        self.show_roc_curve()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rePZdDdJvsc0","colab":{}},"source":["# a = ClassifierReport(y_true, y_pred, y_score, \n","#                       classes_labels=dataset.target_names.tolist())\n","# a.show_all()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGfeq0U2WH9-","colab_type":"code","colab":{}},"source":["# a = ClassifierReport(y_true, y_pred, y_score,\n","#                       classes_labels=classes_labels)\n","# a.show_all()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSWrV0ccWH-D","colab_type":"code","colab":{}},"source":["# print(np.array(a.TP, dtype=int))\n","# print(np.array(a.FP, dtype=int))\n","# print(np.array(a.FN, dtype=int))\n","# print(np.array(a.TN, dtype=int))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PdME3q1dvsdI","colab":{}},"source":["print('Importing Done ...')"],"execution_count":0,"outputs":[]}]}